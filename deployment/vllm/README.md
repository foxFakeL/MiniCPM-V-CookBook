# vLLM

vLLM is a high-performance, easy-to-use engine for LLM inference and serving. For an in-depth overview, see the [documentation](https://docs.vllm.ai/).

This directory contains vLLM deployment guides for different MiniCPM versions. Use the links below to jump to the specific version.

## Versioned Deployment Guides

- MiniCPM-V 4.5: [English](./minicpm-v4_5_vllm.md) | [中文](./minicpm-v4_5_vllm_zh.md)
- MiniCPM-V 4.0: [English](./minicpm-v4_vllm.md) | [中文](./minicpm-v4_vllm_zh.md)
- MiniCPM-o 2.6: [English](./minicpm-o2_6_vllm.md) | [中文](./minicpm-o2_6_vllm_zh.md)
- MiniCPM-V 2.6: [English](./minicpm-v2_6_vllm.md) | [中文](./minicpm-v2_6_vllm_zh.md)
- MiniCPM-V 2.5: [English](./minicpm-v2_5_vllm.md) | [中文](./minicpm-v2_5_vllm_zh.md)
