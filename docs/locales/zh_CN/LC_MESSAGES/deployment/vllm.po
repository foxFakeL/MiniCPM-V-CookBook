# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, OpenBMB
# This file is distributed under the same license as the MiniCPM-o Cookbook
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MiniCPM-o Cookbook \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-26 15:08+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../source/deployment/vllm.md:1 6a584ae48cde4b96806982a799fa65ad
msgid "vLLM"
msgstr "vLLM"

#: ../source/deployment/vllm.md:3 b6cc646437b6419c955b6ee22e90b392
msgid ""
"[vLLM](https://github.com/vllm-project/vllm) is a fast and easy-to-use "
"library for LLM inference and serving. To learn more about vLLM, please "
"refer to the [documentation](https://docs.vllm.ai/en/latest/)."
msgstr ""
"[vLLM](https://github.com/vllm-project/vllm) "
"是一个快速且易于使用的大语言模型（LLM）推理与服务库。如需了解更多关于 vLLM "
"的信息，请参考[官方文档](https://docs.vllm.ai/en/latest/)。"

#: ../source/deployment/vllm.md:6 ae5d362b3b5b442aada7a61f1d905fe5
msgid "1. Environment Setup"
msgstr "1.环境配置"

#: ../source/deployment/vllm.md:8 5f0e950801fa44aeaee58ef89360e0ce
msgid "1.1 Install vLLM"
msgstr "1.1 安装vLLM"

#: ../source/deployment/vllm.md:14 5b6330e8c0ec41ddbc1678e451b8529b
msgid "For video inference, install the video module:"
msgstr "如需进行视频推理，请安装视频模块："

#: ../source/deployment/vllm.md:19 63668301839d4e1d9545338d718161a8
msgid "2. API Service Deployment"
msgstr "2.API 服务部署"

#: ../source/deployment/vllm.md:21 1c840b25fabf4ea3a606188fe0a08ea3
msgid "2.1 Launch API Service"
msgstr "2.1 启动 API 服务"

#: ../source/deployment/vllm.md:27 f86acb15fd6e4ebeaae18b2d290e9fae
msgid "**Parameter Description:**"
msgstr "**参数说明：**"

#: ../source/deployment/vllm.md:28 0141ced959084ab8a1487d7e725ca4ba
msgid "`<model_path>`: Specify the local path to your MiniCPM-V 4.5 model"
msgstr "`<model_path>`：指定本地 MiniCPM-V 4.5 模型路径"

#: ../source/deployment/vllm.md:29 bcef305f159240f4b0e8438b98a7cafa
msgid "`--api-key`: Set the API access key"
msgstr "`--api-key`：设置 API 访问密钥"

#: ../source/deployment/vllm.md:30 cfb721e6fbcd4ef4ac089621ce3213ee
msgid "`--max-model-len`: Set the maximum model length"
msgstr "`--max-model-len`：设置模型最大长度"

#: ../source/deployment/vllm.md:31 bacf4855740440cbbc0e0ae356a69b9d
msgid "`--gpu_memory_utilization`: GPU memory utilization rate"
msgstr "`--gpu_memory_utilization`：GPU 显存利用率"

#: ../source/deployment/vllm.md:33 fecf917f03394c69b52d70b76fd81794
msgid "2.2 Image Inference"
msgstr "2.2 图片推理"

#: ../source/deployment/vllm.md:75 3b76b45bc07541938efa35eb42c491eb
msgid "2.3 Video Inference"
msgstr "2.3 视频推理"

#: ../source/deployment/vllm.md:123 ebf3dee7bd6e49c29f4a973a49701853
msgid "2.4 Thinking and Non-Thinking Modes"
msgstr "2.4 思考与非思考模式"

#: ../source/deployment/vllm.md:125 9cd221b46bc745f5b939595fe17424c6
msgid ""
"The `MiniCPM-V 4.5` model supports thinking before replying, and the "
"thinking mode can be turned on and off by setting the `opanai` request "
"parameters."
msgstr "`MiniCPM-V 4.5` 模型支持在回复前进行思考，可通过设置 `openai` 请求参数开启或关闭思考模式。"

#: ../source/deployment/vllm.md:127 77a52ab428ea48e8aec876707823196d
#, python-brace-format
msgid "`\"chat_template_kwargs\": {\"enable_thinking\": True}`"
msgstr "`\"chat_template_kwargs\": {\"enable_thinking\": True}`"

#: ../source/deployment/vllm.md:129 6eb1abec5a774a109b804b6520145539
msgid ""
"In the reply, thinking and the reply will be separated by the `</think>` "
"tag."
msgstr "在回复内容中，思考内容与正式回复将通过 `</think>` 标签进行分隔。"

#: ../source/deployment/vllm.md:172 0fe9e8d599f24a8f8cc7566da326a15e
msgid "2.5 Multi-turn Conversation"
msgstr "2.5 多轮对话"

#: ../source/deployment/vllm.md:174 1b948790079e452b91d3133bb3aa5f33
msgid "Launch Parameter Configuration"
msgstr "启动参数配置"

#: ../source/deployment/vllm.md:176 35d85a331b7c4f8d84d47a69918f6a48
msgid ""
"For video multi-turn conversations, you need to add the `--limit-mm-per-"
"prompt` parameter when launching vLLM:"
msgstr "对于视频多轮对话，启动 vLLM 时需添加 `--limit-mm-per-prompt` 参数："

#: ../source/deployment/vllm.md:178 ee89417456574769b9ba522112ad046f
msgid "**Video multi-turn conversation configuration (supports up to 3 videos):**"
msgstr "**视频多轮对话配置（最多支持3个视频）：**"

#: ../source/deployment/vllm.md:183 9189921ba78d4b1f9026bedc16f6570b
msgid "**Image and video mixed input configuration:**"
msgstr "**图片与视频混合输入配置：**"

#: ../source/deployment/vllm.md:188 b3086e95a36d485eaaafd9e2b4ea337e
msgid "Multi-turn Conversation Example Code"
msgstr "多轮对话示例代码"

#: ../source/deployment/vllm.md:277 a339b10fede74e90a34172d0900d214b
msgid "3. Offline Inference"
msgstr "3.离线推理"

#: ../source/deployment/vllm.md:349 470a23569f754ea589328386cc8872f9
msgid "Notes"
msgstr "注意事项"

#: ../source/deployment/vllm.md:351 0e983e958f0b4cd4ad9b413d991605db
msgid ""
"**Model Path**: Replace all `<model_path>` in the examples with the "
"actual MiniCPM-V 4.5 model path"
msgstr "**模型路径**：请将示例中的 `<model_path>` 替换为实际的 MiniCPM-V 4.5 模型路径"

#: ../source/deployment/vllm.md:352 35a1a6a14cf64be8bbb2699c34a32a72
msgid ""
"**API Key**: Ensure the API key when launching the service matches the "
"key in the client code"
msgstr "**API 密钥**：请确保启动服务时的 API 密钥与客户端代码中的密钥一致"

#: ../source/deployment/vllm.md:353 f3afb68db3c3489293e173c7b9655615
msgid ""
"**File Paths**: Adjust image and video file paths according to your "
"actual situation"
msgstr "**文件路径**：请根据实际情况调整图片和视频文件路径"

#: ../source/deployment/vllm.md:354 dbf9103914f74513ac0b2e7e47a5c44a
msgid ""
"**Memory Configuration**: Adjust the `--gpu_memory_utilization` parameter"
" appropriately based on GPU memory"
msgstr "**显存配置**：请根据 GPU 显存情况合理调整 `--gpu_memory_utilization` 参数"

#: ../source/deployment/vllm.md:355 5f077309c11745148fecaa56957bbee1
msgid ""
"**Multimodal Limits**: Set appropriate `--limit-mm-per-prompt` parameters"
" when using multi-turn conversations"
msgstr "**多模态限制**：多轮对话时请合理设置 `--limit-mm-per-prompt` 参数"

#~ msgid ""
#~ "Please note that the prebuilt `vllm` "
#~ "has strict dependencies on `torch` and"
#~ " its CUDA versions. Check the note"
#~ " in the official document for "
#~ "installation "
#~ "([link](https://docs.vllm.ai/en/latest/getting_started/installation.html))"
#~ " for more help."
#~ msgstr ""
#~ "请注意，预编译的 `vllm` 对 `torch` 及其 CUDA "
#~ "版本有严格依赖。如需更多帮助，请查阅官方文档中的安装说明（[链接](https://docs.vllm.ai/en/latest/getting_started/installation.html)）。"

#~ msgid ""
#~ "It is easy to build an OpenAI-"
#~ "compatible API service with vLLM, which"
#~ " can be deployed as a server "
#~ "that implements OpenAI API protocol. By"
#~ " default, it starts the server at "
#~ "http://localhost:8000. You can specify the "
#~ "address with --host and --port "
#~ "arguments. Run the command as shown "
#~ "below:"
#~ msgstr ""
#~ "使用 vLLM 可以轻松构建兼容 OpenAI 的 API "
#~ "服务，可作为实现 OpenAI API 协议的服务器部署。默认情况下，服务器会启动在 "
#~ "http://localhost:8000。你可以通过 --host 和 --port "
#~ "参数指定地址。请按如下命令运行："

#~ msgid ""
#~ "We've submitted a PR for MiniCPM-V "
#~ "4.5 to the vLLM repo, and it's "
#~ "currently under review for merging. In"
#~ " the meantime, you can use our "
#~ "code via [this link](https://github.com/tc-"
#~ "mb/vllm/tree/Support-MiniCPM-V-4.5).."
#~ msgstr ""
#~ "我们已向 vLLM 仓库提交了 MiniCPM-V 4.5 的 "
#~ "PR，目前正在审核合并中。在此期间，您可以通过[此链接](https://github.com/tc-mb/vllm/tree"
#~ "/Support-MiniCPM-V-4.5)使用我们的代码。"

